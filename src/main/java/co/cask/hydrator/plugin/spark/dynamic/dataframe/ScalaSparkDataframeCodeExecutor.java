/*
 * Copyright Â© 2018 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package co.cask.hydrator.plugin.spark.dynamic.dataframe;

import co.cask.cdap.api.data.format.StructuredRecord;
import co.cask.cdap.api.data.schema.Schema;
import co.cask.cdap.api.spark.dynamic.CompilationFailureException;
import co.cask.cdap.api.spark.dynamic.SparkInterpreter;
import co.cask.cdap.api.spark.sql.DataFrames;
import co.cask.cdap.etl.api.batch.SparkExecutionPluginContext;
import co.cask.hydrator.plugin.spark.dynamic.RecordToRow;
import co.cask.hydrator.plugin.spark.dynamic.ScalaSparkSink;
import co.cask.hydrator.plugin.spark.dynamic.SparkCompilers;
import org.apache.spark.SparkContext;
import org.apache.spark.SparkFirehoseListener;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.rdd.RDD;
import org.apache.spark.scheduler.SparkListenerApplicationEnd;
import org.apache.spark.scheduler.SparkListenerEvent;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.types.StructType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import javax.annotation.Nullable;
import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.io.StringWriter;
import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;
import java.lang.reflect.ParameterizedType;
import java.lang.reflect.Type;
import java.nio.file.Files;

/**
 * Compiles and executes scala spark code that takes an RDD or Dataframe as input.
 */
public class ScalaSparkDataframeCodeExecutor {

  private static final Logger LOG = LoggerFactory.getLogger(ScalaSparkDataframeCodeExecutor.class);
  private static final String CLASS_NAME_PREFIX = "co.cask.hydrator.plugin.spark.dynamic.dataframe.generated.UserScalaSpark$";
  private static final Class<?> DATAFRAME_TYPE = getDataFrameType();
  private static final Class<?>[][] ACCEPTABLE_PARAMETER_TYPES = new Class<?>[][] {
    { Dataset.class, SparkExecutionPluginContext.class },
    { Dataset.class },
    { DATAFRAME_TYPE, SparkExecutionPluginContext.class},
    { DATAFRAME_TYPE }
  };

  private final String code;
  private final String dependencies;
  private final String methodName;
  private final boolean isVoid;
  private final ThreadLocal<SQLContext> sqlContextThreadLocal = new InheritableThreadLocal<>();

  // A strong reference is needed to keep the compiled classes around
  @SuppressWarnings("FieldCanBeLocal")
  private transient SparkInterpreter interpreter;
  private transient Method method;
  private transient boolean isDataFrame;
  private transient boolean takeContext;

  public ScalaSparkDataframeCodeExecutor(String code, @Nullable String dependencies, String methodName, boolean isVoid) {
    LOG.debug("code :: " + code);
    this.code = code;
    this.dependencies = dependencies;
    this.methodName = methodName;
    this.isVoid = isVoid;
  }

  /**
   * Runs validation to be performed at configure time.
   */
  public void configure(@Nullable Schema inputSchema) {

    SparkInterpreter interpreter = SparkCompilers.createInterpreter();
    if (interpreter != null) {
      File dir = null;
      try {
        if (dependencies != null) {
          dir = Files.createTempDirectory("sparkprogram").toFile();
          SparkCompilers.addDependencies(dir, interpreter, dependencies);
        }
        // We don't need the actual stage name as this only happen in deployment time for compilation check.
        String className = generateClassName("dummy");
        interpreter.compile(generateSourceClass(className));

        // Make sure it has a valid transform method
        Method method = getMethod(interpreter.getClassLoader(), className);

        // If the method takes DataFrame, make sure it has input schema
        if (isDataFrame(method.getParameterTypes()[0]) && inputSchema == null) {
          throw new IllegalArgumentException("Missing input schema for transformation using DataFrame");
        }

      } catch (CompilationFailureException e) {
        throw new IllegalArgumentException(e.getMessage(), e);
      } catch (IOException e) {
        throw new RuntimeException(e);
      } finally {
        SparkCompilers.deleteDir(dir);
      }
    }
  }

  /**
   * Initializes the interpreter
   */
  public void initialize(SparkExecutionPluginContext context) throws Exception {
    String className = generateClassName(context.getStageName());
    interpreter = context.createSparkInterpreter();
    File tempDir = null;
    if (dependencies != null) {
      tempDir = Files.createTempDirectory("sparkprogram").toFile();
      SparkCompilers.addDependencies(tempDir, interpreter, dependencies);
    }
    // Release resources on application completion.
    final File finalTempDir = tempDir;
    SparkFirehoseListener sparkListener = new SparkFirehoseListener() {
      @Override
      public void onEvent(SparkListenerEvent event) {
        if (event instanceof SparkListenerApplicationEnd) {
          LOG.info("Releasing resources on Spark application completion.");
          interpreter.close();
          if (finalTempDir != null) {
            SparkCompilers.deleteDir(finalTempDir);
          }
        }
      }
    };
    // Need to use reflection to find and call the addSparkListener() method, due to incompatible changes
    // between Spark1 (SparkListener) and Spark2 (SparkListenerInterface).
    SparkContext sc = context.getSparkContext();
    for (Method method : sc.getClass().getMethods()) {
      if (method.getName().equals("addSparkListener")) {
        Class<?>[] paramTypes = method.getParameterTypes();
        if (paramTypes.length == 1 && paramTypes[0].isAssignableFrom(sparkListener.getClass())) {
          method.invoke(sc, sparkListener);
          break;
        }
      }
    }

    interpreter.compile(generateSourceClass(className));
    method = getMethod(interpreter.getClassLoader(), className);

    isDataFrame = isDataFrame(method.getParameterTypes()[0]);
    takeContext = method.getParameterTypes().length == 2;

    // Input schema shouldn't be null
    if (isDataFrame && context.getInputSchema() == null) {
      throw new IllegalArgumentException("Input schema must be provided for using DataFrame in Spark Compute");
    }
  }

  public boolean isDataFrame() {
    return isDataFrame;
  }

  /**
   * Execute interpreted code on the given RDD.
   */
  public Dataset execute(SparkExecutionPluginContext context,
                        Dataset input) throws InvocationTargetException, IllegalAccessException {
    // RDD case
//    if (!isDataFrame) {
//      if (takeContext) {
//        return method.invoke(null, input, context);
//      } else {
//        return method.invoke(null, input.rdd());
//      }
//    }

    // DataFrame case
    Schema inputSchema = context.getInputSchema();
    if (inputSchema == null) {
      // Should already been checked in initialize. This is to safeguard in case the call sequence changed in future.
      throw new IllegalArgumentException("Input schema must be provided for using DataFrame in Spark Compute");
    }

//    SQLContext sqlContext = getSQLContext(context.getSparkSession().sqlContext());
//
//    StructType rowType = DataFrames.toDataType(inputSchema);
//    JavaRDD<Row> rowRDD = javaRDD.map(new RecordToRow(rowType));
//
//    Object dataFrame = createDataFrame(sqlContext, rowRDD, rowType);

    if(takeContext){
      return (Dataset) method.invoke(null, input, context);
    }else{
      return (Dataset) method.invoke(null, input);
    }

  }

  private String generateSourceClass(String className) {
    StringWriter writer = new StringWriter();

    try (PrintWriter sourceWriter = new PrintWriter(writer, false)) {
      sourceWriter.println("package " + className.substring(0, className.lastIndexOf('.')));
      // Includes some commonly used imports.
      sourceWriter.println("import co.cask.cdap.api.data.format._");
      sourceWriter.println("import co.cask.cdap.api.data.schema._");
      sourceWriter.println("import co.cask.cdap.etl.api.batch._");
      sourceWriter.println("import org.apache.spark._");
      sourceWriter.println("import org.apache.spark.api.java._");
      sourceWriter.println("import org.apache.spark.rdd._");
      sourceWriter.println("import org.apache.spark.sql._");
      sourceWriter.println("import org.apache.spark.SparkContext._");
      sourceWriter.println("import scala.collection.JavaConversions._");
      sourceWriter.println("object " + className.substring(className.lastIndexOf('.') + 1) + " {");
      sourceWriter.println(code);
      sourceWriter.println("}");
    }

    return writer.toString();
  }

  /**
   * Validates the given type is {@code RDD<StructuredRecord>}.
   */
  private void validateRDDType(Type rddType, String errorMessage) {
    if (!(rddType instanceof ParameterizedType)) {
      throw new IllegalArgumentException(errorMessage);
    }
    if (!RDD.class.equals(((ParameterizedType) rddType).getRawType())) {
      throw new IllegalArgumentException(errorMessage);
    }

    Type[] typeParams = ((ParameterizedType) rddType).getActualTypeArguments();
    if (typeParams.length < 1 || !typeParams[0].equals(StructuredRecord.class)) {
      throw new IllegalArgumentException(errorMessage);
    }
  }

  private SQLContext getSQLContext(SparkContext sc) {
    SQLContext sqlContext = sqlContextThreadLocal.get();
    if (sqlContext != null && !sqlContext.sparkContext().isStopped()) {
      return sqlContext;
    }

    synchronized (this) {
      sqlContext = sqlContextThreadLocal.get();
      if (sqlContext == null || sqlContext.sparkContext().isStopped()) {
        sqlContext = new SQLContext(sc);
        sqlContextThreadLocal.set(sqlContext);
      }
    }

    return sqlContext;
  }

  private Method getMethod(ClassLoader classLoader, String className) {
    // Use reflection to load the class and get the transform method
    try {
      Class<?> computeClass = classLoader.loadClass(className);

      // Find which method to call
      Method method = null;
      for (Class<?>[] paramTypes : ACCEPTABLE_PARAMETER_TYPES) {
        method = tryFindMethod(computeClass, methodName, paramTypes);
        if (method != null) {
          break;
        }
      }

//      String returnType1 = isVoid ? "Unit" : "RDD[StructuredRecord]";
//      String returnType2 = isVoid ? "Unit" : "DataFrame[StructuredRecord]";
      String returnType3 = isVoid ? "Unit" : "Dataset[Row]";
      if (method == null) {
        throw new IllegalArgumentException(String.format(
          "Missing a `%s` method that has signature in one of the following form\n" +
            "def %s(dataset: Dataset) : %s\n" +
            "def %s(dataset: Dataset, context: SparkExecutionPluginContext) : %s",
          methodName, methodName, returnType3, methodName, returnType3));
      }

      Type[] parameterTypes = method.getGenericParameterTypes();
      boolean isDataFrame = isDataFrame(parameterTypes[0]);

      // The first parameter should be of type RDD[StructuredRecord] if it takes RDD
      if (!isDataFrame) {
        validateRDDType(parameterTypes[0], String.format(
          "The first parameter of the '%s' method should have type as 'RDD[StructuredRecord]'", methodName));
      }

      // If it has second parameter, then must be SparkExecutionPluginContext
      if (parameterTypes.length == 2 && !SparkExecutionPluginContext.class.equals(parameterTypes[1])) {
        throw new IllegalArgumentException(String.format(
          "The second parameter of the '%s' method should have type as SparkExecutionPluginContext", methodName));
      }

      // The return type of the method must be Unit
      if (isVoid) {
        if (!void.class.equals(method.getReturnType())) {
          throw new IllegalArgumentException(String.format("The return type of the '%s' method should be 'Unit'",
                                                           methodName));
        }
      } else {
        // The return type of the method must be RDD[StructuredRecord] if it takes RDD
        // Or it must be DataFrame if it takes DataFrame
        if (isDataFrame) {
          if (!isDataFrame(method.getGenericReturnType())) {
            throw new IllegalArgumentException(String.format(
              "The return type of the '%s' method should be 'DataFrame'", methodName));
          }
        } else {
          validateRDDType(method.getGenericReturnType(), String.format(
            "The return type of the '%s' method should be 'RDD[StructuredRecord]'", methodName));
        }
      }

      method.setAccessible(true);
      return method;
    } catch (ClassNotFoundException e) {
      // This shouldn't happen since we define the class name.
      throw new IllegalArgumentException(e);
    }
  }

  @Nullable
  private Method tryFindMethod(Class<?> cls, String name, Class<?>...parameterTypes) {
    try {
      return cls.getDeclaredMethod(name, parameterTypes);
    } catch (NoSuchMethodException e) {
      return null;
    }
  }

  private String generateClassName(String stageName) {
    // Hex encode any non-alphanumeric character in the stage name
    StringBuilder nameBuilder = new StringBuilder(CLASS_NAME_PREFIX);
    for (char c : stageName.toCharArray()) {
      if (Character.isLetter(c) || Character.isDigit(c)) {
        nameBuilder.append(c);
      } else {
        nameBuilder.append(String.format("%02X", (int) c));
      }
    }
    return nameBuilder.toString();
  }

  /**
   * Returns whether the given {@link Type} is a DataFrame type.
   */
  private boolean isDataFrame(Type type) {
    if (DATAFRAME_TYPE == null) {
      return false;
    }
    if (type instanceof Class) {
      return ((Class<?>) type).isAssignableFrom(DATAFRAME_TYPE);
    }
    if (type instanceof ParameterizedType) {
      return isDataFrame(((ParameterizedType) type).getRawType());
    }
    return false;
  }

  private Object createDataFrame(SQLContext sqlContext, JavaRDD<Row> rdd, StructType type) {
    try {
      return sqlContext.getClass()
        .getMethod("createDataFrame", JavaRDD.class, StructType.class)
        .invoke(sqlContext, rdd, type);
    } catch (NoSuchMethodException e) {
      throw new RuntimeException("Unable to find SQLContext.createDataFrame(JavaRDD, StructType) method", e);
    } catch (IllegalAccessException e) {
      throw new RuntimeException("No permission to invoke SQLContext.createDataFrame(JavaRDD, StructType) method", e);
    } catch (InvocationTargetException e) {
      throw new RuntimeException("Failed to invoke SQLContext.createDataFrame(JavaRDD, StructType) method", e);
    }
  }

  @Nullable
  private static Class<?> getDataFrameType() {
    try {
      return ScalaSparkSink.class.getClassLoader().loadClass("org.apache.spark.sql.Dataset");
    } catch (ClassNotFoundException e) {
      LOG.error("ClassNotFound `org.apache.spark.sql.Dataset` ");
      return null;
    }
  }
}
